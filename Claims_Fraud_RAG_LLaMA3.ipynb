{'cells': [{'cell_type': 'markdown', 'metadata': {}, 'source': ['# 🔍 Claims Fraud Detection using RAG + LLaMA3 + FAISS (Together.ai)\n', '- Embedding model: `BAAI/bge-small-en-v1.5`\n', '- LLM model: `meta-llama/llama-3-8b-instruct`\n', '- Vector Store: FAISS\n', '- Platform: Together.ai\n', '- Data: Unstructured insurance claim reports\n']}, {'cell_type': 'code', 'execution_count': None, 'metadata': {}, 'outputs': [], 'source': ['# ✅ Install required libraries\n', '!pip install openai faiss-cpu tiktoken numpy pandas']}, {'cell_type': 'code', 'execution_count': None, 'metadata': {}, 'outputs': [], 'source': ['# ✅ Setup Together.ai client (OpenAI-compatible)\n', 'import os\n', 'from openai import OpenAI\n', '\n', '# Replace with your actual Together.ai API key\n', 'os.environ["TOGETHER_API_KEY"] = "your_together_api_key_here"\n', 'client = OpenAI(\n', '    base_url="https://api.together.xyz/v1",\n', '    api_key=os.environ["TOGETHER_API_KEY"]\n', ')']}, {'cell_type': 'code', 'execution_count': None, 'metadata': {}, 'outputs': [], 'source': ['# ✅ Load unstructured claims data\n', 'def load_claims(file_path):\n', '    with open(file_path, "r", encoding="utf-8") as f:\n', '        data = f.read()\n', '    return data.strip().split("---\\n")\n', '\n', 'claims = load_claims("claims_unstructured.txt")\n', 'print(f"Loaded {len(claims)} claim documents.")']}, {'cell_type': 'code', 'execution_count': None, 'metadata': {}, 'outputs': [], 'source': ['# ✅ Get embeddings for claims using Together.ai\n', 'import numpy as np\n', 'from typing import List\n', '\n', 'def get_embedding(text: str, model="BAAI/bge-small-en-v1.5") -> List[float]:\n', '    response = client.embeddings.create(\n', '        input=[text],\n', '        model=model\n', '    )\n', '    return response.data[0].embedding\n', '\n', 'claim_embeddings = [get_embedding(c) for c in claims]']}, {'cell_type': 'code', 'execution_count': None, 'metadata': {}, 'outputs': [], 'source': ['# ✅ Store embeddings in FAISS index\n', 'import faiss\n', '\n', 'dimension = len(claim_embeddings[0])\n', 'index = faiss.IndexFlatL2(dimension)\n', 'index.add(np.array(claim_embeddings).astype("float32"))\n', 'print("FAISS index ready.")']}, {'cell_type': 'code', 'execution_count': None, 'metadata': {}, 'outputs': [], 'source': ['# ✅ RAG functions: retrieve and generate\n', 'def retrieve_claims(query, k=5):\n', '    query_embedding = np.array(get_embedding(query)).astype("float32").reshape(1, -1)\n', '    distances, indices = index.search(query_embedding, k)\n', '    return [claims[i] for i in indices[0]]\n', '\n', 'def generate_answer_with_rag(query):\n', '    context_docs = retrieve_claims(query)\n', '    context_text = "\\n---\\n".join(context_docs)\n', '    \n', '    prompt = f"""You are a fraud detection analyst.\n', "Use the following insurance claim reports to answer the user's question.\n", '\n', 'Context:\n', '{context_text}\n', '\n', 'Question:\n', '{query}\n', '\n', 'Answer:"""\n', '\n', '    completion = client.chat.completions.create(\n', '        model="meta-llama/llama-3-8b-instruct",\n', '        messages=[\n', '            {"role": "user", "content": prompt}\n', '        ],\n', '        temperature=0.3,\n', '        max_tokens=800\n', '    )\n', '    return completion.choices[0].message.content']}, {'cell_type': 'code', 'execution_count': None, 'metadata': {}, 'outputs': [], 'source': ['# ✅ Ask a question to test the system\n', 'question = "What patterns indicate fraudulent insurance claims?"\n', 'answer = generate_answer_with_rag(question)\n', 'print("Q:", question)\n', 'print("A:", answer)']}], 'metadata': {'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': ''}}, 'nbformat': 4, 'nbformat_minor': 5}